# 시퀀스 투 시퀀스(seq2seq) 개념
- 앞서 RNN으로 다대일 구조로 텍스트 분류를 품
- seq2seq는 인코더 - 디코더로 이루어진 구조임
    - 시퀀스-투-시퀀스(Sequence-to-Sequence, seq2seq)는 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 다양한 분야에서 사용되는 모델 (챗봇, 기계번역) 외  (내용요약, STT)
---
## 시퀀스-투-시퀀스 (인코더/디코더)
- **seq2seq**는 크게 인코더와 디코더라는 두 개의 모듈로 구성됩니다. 
인코더는 입력 문장의 모든 단어들을 순차적으로 입력받은 뒤에 마지막에 이 모든 단어 정보들을 압축해서 하나의 벡터로 만드는데, 이를 **컨텍스트 벡터**(context vector)라고 합니다. 입력 문장의 정보가 하나의 컨텍스트 벡터로 모두 압축되면 인코더는 컨텍스트 벡터를 디코더로 전송합니다. 디코더는 컨텍스트 벡터를 받아서 번역된 단어를 한 개씩 순차적으로 출력합니다.
- **컨텍스트 벡터** : 여러 정보가 담긴 수백차원이 압축된 벡터

- 인코더 아키텍처와 디코더 아키텍처의 내부는 사실 두 개의 RNN 아키텍처 입니다. 입력 문장을 받는 RNN 셀을 인코더라고 하고, 출력 문장을 출력하는 RNN 셀을 디코더라고 합니다. 

1. 우선 인코더에 입력문장이 단어 토큰화를 통해 단어 단위로 쪼개짐
2. 토큰 각각은 RNN셀의 각 시점의 입력이 됨.
3. 인코더RNN셀의 마지막 시점의 은닉상태를 디코더 RNN셀로 넘김 이를 **컨텍스트**벡터라고 함
4. 이 벡터는 디코더 RNN셀의 첫번째 은닉 상태에 사용된다.

5. 디코더는 기본적으로 RNNLM(RNN Language Model)임
6. 디코더에 초기 입력으로 <sos>가 들어감. 이게 들어가면 다음에 등장할 확률이 높은 단어를 예측-> 첫번째 시점의 디코더 RNN셀은다음에 등장할 단어로 가정(번역된 단어의 첫단어)를 예측하게된다. 또 그다음 단어를 예측 예측 예측 연속으로 예측함.
7. 이 반복은 <eos> 가 다음 단어로 예측될 때까지 반복된다.
8. 훈련과정과 테스트 과정의 작동방식이 조금다름
    - 훈련: 디코더에게 인코더가 보낸 컨텍스트 벡터와 실제정담상황인 <sos>번역된단어~ 가 입력되면 번역된단어<eos>가 나와야한다고 정답을 알려주면서 훈련함 이는 **교사강요**라고 불러지기도함.
    
    - 테스트: 디코더는 오직 컨텍스트 벡터와 <sos>만을 입력으로 받은 후 다음에 올 단어를 예측, 그 단어를 다음 시점의 RNN 셀의 입력으로 넣는 행위를 반복함.
---
## 임베딩층 
![이미지](https://wikidocs.net/images/page/24996/%EB%8B%A8%EC%96%B4%ED%86%A0%ED%81%B0%EB%93%A4%EC%9D%B4.PNG)
● 기계는 텍스트보다 숫자를 잘 처리합니다. 자연어 처리에서 텍스트를 벡터로 바꾸는 방법으로 주로 워드 임베딩이 사용된다고 설명한 바 있습니다. 즉, seq2seq에서 사용되는 모든 단어들은 임베딩 벡터로 변환 후 입력으로 사용됩니다. 위 그림은 모든 단어에 대해서 임베딩 과정을 거치게 하는 단계인 임베딩 층(embedding layer)의 모습을 보여줍니다.
- 그리고 각단어는 임베딩 벡터를 갖는다. 

- 하나의 셀은 각 시점마다 두개의 입력을 받는다.

1. 현재시점 t에 RNN은
2. t-1(은닉 상태)와 t(현재시점)의 입력 벡터를 입력받고, t에서의 은닉 상태를 생성.
3. 이 상태에서 위에 다른 은닉,출력 층이 존재하면 위로 보내거나, 값을 무시할 수 있음.
4. 그리고 다음시점에 해당되는 t+1 셀의 입력으로 현재의  t의 은닉상태를 입력으로 보냄

● 출력 단어로 나올 수 있는 단어들은 다양한 단어들이 있습니다. seq2seq 모델은 선택될 수 있는 모든 단어들로부터 하나의 단어를 골라서 예측해야 합니다. 이를 예측하기 위해서 쓸 수 있는 함수로는 뭐가 있을까요? 바로 소프트맥스 함수입니다. 디코더에서 각 시점(time step)의 RNN 셀에서 출력 벡터가 나오면, 해당 벡터는 소프트맥스 함수를 통해 출력 시퀀스의 각 단어 별 확률값을 반환하고, 디코더는 출력 단어를 결정합니다.

---
## 어텐션 매커니즘
- seq2seq의 한계로 인해 고안된 어텐션
- RNN에 기반한 seq2seq 모델에는 크게 두 가지 문제가 있습니다.    
    - 첫째, 하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 하니까 정보 손실이 발생합니다
    - 둘째, RNN의 고질적인 문제인 기울기 소실(vanishing gradient) 문제가 존재합니다.
- 결국 이는 기계 번역 분야에서 입력 문장이 길면 번역 품질이 떨어지는 현상으로 나타났습니다. 
- 정확도가 떨어지는 것을 보정해주기 위해 어텐션이 고려됨

- 어텐션의 기본 아이디어는 디코더에서 출력 단어를 예측하는 매시점마다, 인코더에서의 전처리 문장을 다시 한번 참고함.
● 어텐션 메커니즘을 언급하기 전에 컴퓨터공학의 많은 분야에서 사용되는 Key-Value로 구성되는 자료형에 대해서 잠깐 언급하겠습니다. 가령, 이 책의 주 언어로 사용되는 파이썬에도 Key-Value로 구성되는 자료형인 딕셔너리(Dict) 자료형이 존재합니다. 파이썬의 딕셔너리 자료형은 키(Key)와 값(Value)이라는 두 개의 쌍으로 구성되는데, 키를 통해서 맵핑된 값을 찾아낼 수 있다는 특징을 갖고있습니다.

● 어텐션을 함수로 표현하면 주로 다음과 같이 표현됩니다. **Attention(Q, K, V) = Attention Value**

- 어텐션 함수는 주어진 '쿼리(Query)'에 대해서 모든 '키(Key)'와의 유사도를 각각 구합니다. 
그리고 구해낸 이 유사도를 키와 맵핑되어있는 각각의 '값(Value)'에 반영해줍니다. 그리고 유사도가 반영된 '값(Value)'을 모두 더해서 리턴합니다. 여기서는 이를 어텐션 값(Attention Value)이라고 하겠습니다.

![어텐션](https://wikidocs.net/images/page/22893/dotproductattention1_final.PNG)

● 위 그림은 디코더의 세번째 LSTM 셀에서 출력 단어를 예측할 때, 어텐션 메커니즘을 사용하는 모습을 보여줍니다. 
디코더의 첫번째, 두번째 LSTM 셀은 이미 어텐션 메커니즘을 통해 je와 suis를 예측하는 과정을 거쳤다고 가정합니다. 어텐션 메커니즘에 대해 상세히 설명하기 전에 위의 그림을 통해 전체적인 개요만 이해해보겠습니다. 디코더의 세번째 LSTM 셀은 출력 단어를 예측하기 위해서 인코더의 모든 입력 단어들의 정보를 다시 한번 참고하고자 합니다. 중간 과정에 대한 설명은 현재는 생략하고 여기서 주목할 것은 인코더의 소프트맥스 함수입니다.

- 소프트맥스 함수를 통해나온 결과값은 단어 출력단어를 예측할때 얼마나 도움이 될지 정도를 수치화한 값임. 위의 초록색 삼각형이 각 수치를 합하여 전송한 정보임.

---

## 트랜스포머
- 기존 seq2seq 모델의 한계는 컨텍스트 벡터로 압축할때 그 과정에서 시퀀스의 정보가 일부 손실된다는 것이다. 그래서 이를 보조하기 위해 어텐션을 사용
- 근데 어텐션을 RNN을 보조하는 용도로만 사용하는게 아닌 어텐션만으로 인코더와 디코더를 만든다면?
1. 구조: 기존 모델처럼 인코더와 디코더가 있다. 이전 모델처럼 t개의 시점을 가지는 구조가 아닌 인코더와 디코더라는 단위가 N개로 구성되는 구조로 구성
2. 구조가 실행되는데 이전 모델처럼 sos 로 시작해서 eos 가나올때 까지 연산ㅇ을 진행된다. RNN은 사용하지 않지만 인코더-디코더가 유지됨
3. 임베딩 벡터를 받을때 각 단어의 임베딩 벡터를 그냥 받는게 아닌 조정된 벡터값을 받는다.
4. 트랜스포머는 단어를 순서대로 입력받아 처리하는게 아니므로 각단어의 위치정보를 얻기위해 포지셔널 인코딩을한다. 포지셔널 인코딩+ 인베딩 벡터
